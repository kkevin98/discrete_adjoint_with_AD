{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../src/RadialBasisFunction.jl\")\n",
    "include(\"../src/DiscreteAdjoint.jl\")\n",
    "include(\"../src/GradientDescentTools.jl\")\n",
    "\n",
    "using LinearAlgebra, Plots\n",
    "using .RadialBasisFunction\n",
    "using .DiscreteAdjoint\n",
    "using .GradientDescentTools\n",
    "\n",
    "const dir_for_figures = \"../results/figures/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjoint method applied to RBF approximation\n",
    "\n",
    "In this notebook, unlike the previous one, the parameter that we want to optimize, $q$ also appear explicitly inside the objecitve function $J$; so in this case we have $J(u,q)$ instead of simply $J(u)$  \n",
    "The optimization through th eadjoint change in the fact that now we need one more term to compute $\\frac{d}{dq} J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed some paramters, we obtain an approximated field solving the steady-state heat equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 0, 2π\n",
    "N = 40\n",
    "X = range(a,b, length=N)\n",
    "boundary_idx = [1, N]\n",
    "X_boundary = X[boundary_idx]\n",
    "internal_idx = 2:(N-1)\n",
    "X_internal = X[internal_idx]\n",
    "\n",
    "\n",
    "u(x) = sin.(x)   # Not known in advance\n",
    "q(x) = -sin.(x)  # Known in advance (param)\n",
    "\n",
    "boundary_conditions = u(X_boundary)\n",
    "\n",
    "\n",
    "φ(r) = r .^ 3  # r = |x-x_i| ≥ 0\n",
    "ddφ(r) = 6*r   #second derivative d²/dx² (because the differential problem deals with u\" = d²u/dx²)\n",
    "\n",
    "\n",
    "n_points_in_stencil = 7\n",
    "\n",
    "\n",
    "C = global_matrix(X, φ, ddφ, n_points_in_stencil)\n",
    "u_approximated = solve_RBF(C, X, internal_idx, boundary_idx, boundary_conditions, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find the field $u_{opt}$ that minimize a cost function $J(u,q)$. We start by defining the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the optimization is carried out only on internal points (in fact boundary poiints are conditioned)...\n",
    "C_internal = C[internal_idx, internal_idx]\n",
    "\n",
    "# Defininig J (obj function)\n",
    "u_target(x) = (x .- a) .* (b .- x)\n",
    "u_target_num = u_target(X_internal)\n",
    "c_1, c_2 = 0.4, 0.6\n",
    "J(u_num, q_num) = c_1*norm(u_num - u_target_num)^2 + c_2*norm(q_num)^2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up everything that is required for the optimization with the adjoint method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial derivatives of J (used to compute the total derivative respect q)\n",
    "dJdu(u_num, q_num) = 2*c_1*(u_num - u_target_num)\n",
    "dJdq(u_num, q_num) = 2*c_2*q_num\n",
    "\n",
    "# Defining the initial values of the constraint with the ones obtained from the RBF approximation\n",
    "u_RBF = u_approximated\n",
    "f_RBF = C[internal_idx, boundary_idx] * boundary_conditions\n",
    "q_RBF = C_internal*u_RBF + f_RBF\n",
    "constraint = RBFEquation(C_internal, u_RBF, q_RBF, f_RBF)\n",
    "\n",
    "N_iter::Int64 = 100    # Number of optimization cycles\n",
    "eval_freq::Int64 = 1   # Frequency of evaluation of the objective function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the sougth parameters $q$ that leads to the desired field $u_{opt}$ we use different iterative gradient methods in the discrete adjoint optimization:\n",
    "- Barzilai-Borwein with long steps\n",
    "- Barzilai-Borwein with short steps\n",
    "- Standard gradient update with fixed steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_opt_BB_long_num, u_opt_BB_long_num, q_results_BB_long, u_results_BB_long, steps_to_eval = adjoint_opt_BB(dJdu, dJdq, constraint, N_iter, eval_freq, BB_long_step)\n",
    "\n",
    "q_opt_BB_short_num, u_opt_BB_short_num, q_results_BB_short, u_results_BB_short = adjoint_opt_BB(dJdu, dJdq, constraint, N_iter, eval_freq, BB_short_step)\n",
    "\n",
    "step_size(∇, n) = 1/norm(∇)\n",
    "q_opt_grad_fixed_steps_num, u_opt_grad_fixed_steps_num, q_results_grad_fixed_steps, u_results_grad_fixed_steps = adjoint_opt(dJdu, dJdq, constraint, N_iter, eval_freq, step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Barzilai-Borwein long steps\")\n",
    "println(\"Fitness value: $(J(u_opt_BB_long_num, q_opt_BB_long_num))\")\n",
    "println()\n",
    "\n",
    "println(\"Barzilai-Borwein short steps\")\n",
    "println(\"Fitness value: $(J(u_opt_BB_short_num, q_opt_BB_short_num))\")\n",
    "println()\n",
    "\n",
    "println(\"Standard gradient fixed steps\")\n",
    "println(\"Fitness value: $(J(u_opt_grad_fixed_steps_num, q_opt_grad_fixed_steps_num))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the value of $J$ at each step of the optimization..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_results_BB_long = [J(u,q) for (u,q) in zip(eachcol(u_results_BB_long), eachcol(q_results_BB_long))]\n",
    "J_results_BB_short = [J(u,q) for (u,q) in zip(eachcol(u_results_BB_short), eachcol(q_results_BB_short))]\n",
    "J_results_grad_fixed_steps = [J(u,q) for (u,q) in zip(eachcol(u_results_grad_fixed_steps), eachcol(q_results_grad_fixed_steps))];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plot(steps_to_eval, J_results_BB_long, label=\"Barzilai-Borwein long steps\")\n",
    "plot!(steps_to_eval, J_results_BB_short, label=\"Barzilai-Borwein short steps\")\n",
    "plot!(steps_to_eval, J_results_grad_fixed_steps, label=\"Gradient fixed steps\")\n",
    "\n",
    "title!(\"J(u,q) vs. number of iterations\")\n",
    "xlabel!(\"Iteration\")\n",
    "ylabel!(\"J\")\n",
    "plot!(legend=:topright)\n",
    "\n",
    "# To limit the axis\n",
    "# ylims!(0, 200)\n",
    "xlims!(0, 50)\n",
    "\n",
    "savefig(dir_for_figures*\"J(u,q)_vs_iter_gradien_steps_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "\n",
    "- *Standard gradient fixed step*s does not converge\n",
    "- *Barzila-Borwein short steps* is the fastest to reach convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
